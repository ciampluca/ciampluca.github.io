<!DOCTYPE html>
<html lang="en" ng-app="AdvApp">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning Pedestrian Detection from Virtual Worlds</title>
    
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.6.4/angular.min.js" crossorigin="anonymous"></script>
    <script src="ng-infinite-scroll.min.js" type="application/javascript"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    
    <!--
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-97320150-1', 'auto');
    ga('send', 'pageview'); 

    
    </script> -->
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">    
    <!-- Fonts! -->
    <link href="http://fonts.googleapis.com/css?family=Roboto:400,300" rel="stylesheet" type="text/css">
    
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            font-weight: 200;
            font-size: 16px;
            text-align: justify;
        }
        h1 { text-align: center; margin-top: 20px; }
	h2 { margin-bottom: 20px; }
        .container > * { margin-bottom: 50px }
        #table-container { margin-top: 50px; height: 600px; overflow: auto; }
        #table-container tbody td { vertical-align: middle; }
        #table-container tbody td.class-text { width: 27%; text-align: left; }
        .score { font-weight: bold; text-align: center; }
        .thumb div {    
            width: 120px;
            height: 120px;
            display: inline-block;
            background-position: center center;
            background-size: cover;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
            border: 2px;
            border-color: #bbbbbb;
            border-radius: 4px;
            box-shadow: 0 0 2px 2px rgba(0, 140, 186, 0.7);
        }

        .samples-footer {
            width: 100%;
            padding: 10px;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.16),0 2px 10px 0 rgba(0,0,0,0.12) !important;
            background-color: #f1f1f1fa;
            display: inline-block;
        }
        .samples-footer a{
            float: right;
        }
	li {margin-bottom: 10px}

        .disabledLink {
            pointer-events:none;
        }

	@media (max-width: 400px) {
    		.center {
        		width: 100%;
    		}
	}

	ul {list-style-type: circle; }

	pre {
		background-color: rgb(246, 248, 250);
		border-bottom-left-radius: 3px;
		border-bottom-right-radius: 3px;
		border-top-left-radius: 3px;
		border-top-right-radius: 3px;
		box-sizing: border-box;
		color: rgb(36, 41, 46);
		font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
		font-size: 13.6px;
		line-height: 19.7167px;
		margin-bottom: 0px;
		margin-top: 20px;
		overflow: auto;
		overflow-wrap: normal;
		overflow-x: auto;
		overflow-y: auto;
		padding-left: 35px;
		padding-bottom: 35px;
		padding-right: 35px;
	}

	code {
		background-attachment: scroll;
		background-clip: border-box;
		background-color: rgba(0, 0, 0, 0);
		background-image: none;
		background-origin: padding-box;
		background-position: 0% 0%;
		background-position-x: 0%;
		background-position-y: 0%;
		background-repeat: repeat;
		background-size: auto;
		border-bottom-color: rgb(36, 41, 46);
		border-bottom-left-radius: 3px;
		border-bottom-right-radius: 3px;
		border-bottom-style: none;
		border-bottom-width: 0px;
		border-image-outset: 0;
		border-image-repeat: stretch;
		border-image-slice: 100%;
		border-image-source: none;
		border-image-width: 1;
		border-left-color: rgb(36, 41, 46);
		border-left-style: none;
		border-left-width: 0px;
		border-right-color: rgb(36, 41, 46);
		border-right-style: none;
		border-right-width: 0px;
		border-top-color: rgb(36, 41, 46);
		border-top-left-radius: 3px;
		border-top-right-radius: 3px;
		border-top-style: none;
		border-top-width: 0px;
		box-sizing: border-box;
		color: rgb(36, 41, 46);
		display: inline;
		font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
		font-size: 13.6px;
		line-height: 19.7167px;
		margin-bottom: 0px;
		margin-left: 0px;
		margin-right: 0px;
		margin-top: 0px;
		overflow: visible;
		overflow-wrap: normal;
		overflow-x: visible;
		overflow-y: visible;
		padding-bottom: 0px;
		padding-left: 0px;
		padding-right: 0px;
		padding-top: 0px;
		white-space: pre;
		word-break: normal;
	}
	</style>
</head>
<body>
    <div class="container">
        <h1>
            Learning Pedestrian Detection from Virtual Worlds<br>
            <small>
                <a href="https://scholar.google.it/citations?user=dCjyf-8AAAAJ&hl=it">Luca Ciampi</a>,
		        <a href="https://scholar.google.it/citations?user=g-UGCd8AAAAJ&hl=en">Nicola Messina</a>,
                <a href="https://scholar.google.it/citations?user=4Vr1dSQAAAAJ&hl=en">Fabrizio Falchi</a>,
                <a href="https://scholar.google.it/citations?user=sbFBI4IAAAAJ&hl=en">Claudio Gennaro</a>,
		        <a href="https://scholar.google.it/citations?user=dXcskhIAAAAJ&hl=en">Giuseppe Amato</a>,
            </small>
        </h1>
        
        
        <div class="abstract">
            <h2>Abstract</h2>
            <p> Pedestrian detection through Computer Vision is a building block for a multitude of applications. Recently, there was an increasing interest in Convolutional Neural Network-based architectures for the execution of such a task. One of these supervised networks' critical goals is to generalize the knowledge learned during the training phase to new scenarios with different characteristics. A suitably labeled dataset is essential to achieve this purpose. The main problem is that manually annotating a dataset usually requires a lot of human effort, and it is costly. To this end, we introduce <b>ViPeD</b> (Virtual Pedestrian Dataset), a new synthetically generated set of images collected with the highly photo-realistic graphical engine of the video game GTA V - Grand Theft Auto V, where annotations are automatically acquired. However, when training solely on the synthetic dataset, the model experiences a Synthetic2Real Domain Shift leading to a performance drop when applied to real-world images. To mitigate this gap, we propose two different Domain Adaptation techniques suitable for the pedestrian detection task, but possibly applicable to general object detection. Experiments show that the network trained with ViPeD can generalize over unseen real-world scenarios better than the detector trained over real-world data, exploiting the variety of our synthetic dataset. Furthermore, we demonstrate that with our Domain Adaptation techniques, we can reduce the Synthetic2Real Domain Shift, making closer the two domains and obtaining a performance improvement when testing the network over the real-world images.
            </p>
        </div>
        
	<div class="container">
	  <div class="row">
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_1_bbs.png"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_2_bbs.png"></div>
	    <div class="w-100"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_3_bbs.png"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_4_bbs.png"></div>
	  </div> 
	</div>


        <div class="paper">
            <h2>Papers</h2>
		<ul>
			<li><b>Learning Pedestrian Detection from Virtual Worlds</b> (<a href="ICIAP__19___Learning_Pedestrian_Detection_from_Virtual_Worlds.pdf"> Download</a>, 3.3MB). The paper has been presented at <a href="https://event.unitn.it/iciap2019/">ICIAP 2019</a>.
			</li>
			<li><b>Virtual to Real adaptation of Pedestrian Detectors</b> <a href="https://arxiv.org/pdf/2001.03032.pdf"> (Pre-Print ArXiv Download</a>, 7.7MB). This is the extended version of the first paper. It contains a wider experimental section and introduces experiments with a different supervised domain-adaptation approach.
			</li>
		</ul>
        </div>

        <div class="dataset">
            <h2>Dataset and Code</h2>
                <p>This dataset is an extension of the <a href="http://imagelab.ing.unimore.it/jta">JTA</a> (Joint Track Auto) dataset.</p>
		<p>
                <a href="http://aimir.isti.cnr.it/viped/">Here</a> you can find more details about the ViPeD dataset, together with the download link.
                </p>
		<p> The code for training and evaluating Faster-RCNN with our method is available in our <a href="https://github.com/ciampluca/Virtual-to-Real-Pedestrian-Detection">GitHub repository</a>.
        </div>
        <div class="cite">
	    <h2>Cite our work</h2>
		If you find this work or code useful for your research, please cite the following:
		<pre><code>
@inproceedings{amato2019learning,
  title={Learning pedestrian detection from virtual worlds},
  author={Amato, Giuseppe and Ciampi, Luca and Falchi, Fabrizio and Gennaro, Claudio and Messina, Nicola},
  booktitle={International Conference on Image Analysis and Processing},
  pages={302--312},
  year={2019},
  organization={Springer}
}
@misc{ciampi2020virtual,
    title={Virtual to Real adaptation of Pedestrian Detectors for Smart Cities},
    author={Luca Ciampi and Nicola Messina and Fabrizio Falchi and Claudio Gennaro and Giuseppe Amato},
    year={2020},
    eprint={2001.03032},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</code></pre>
	</div>
        <div class="acks">
            <p>
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Jetson TX2 used for this research.</p>
        </div>
    </div>
</body>
</html>

