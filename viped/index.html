<!DOCTYPE html>
<html lang="en" ng-app="AdvApp">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning Pedestrian Detection from Virtual Worlds</title>
    
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.6.4/angular.min.js" crossorigin="anonymous"></script>
    <script src="ng-infinite-scroll.min.js" type="application/javascript"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    
    <!--
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-97320150-1', 'auto');
    ga('send', 'pageview'); 

    
    </script> -->
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">    
    <!-- Fonts! -->
    <link href="http://fonts.googleapis.com/css?family=Roboto:400,300" rel="stylesheet" type="text/css">
    
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            font-weight: 200;
            font-size: 16px;
            text-align: justify;
        }
        h1 { text-align: center; margin-top: 20px; }
	h2 { margin-bottom: 20px; }
        .container > * { margin-bottom: 50px }
        #table-container { margin-top: 50px; height: 600px; overflow: auto; }
        #table-container tbody td { vertical-align: middle; }
        #table-container tbody td.class-text { width: 27%; text-align: left; }
        .score { font-weight: bold; text-align: center; }
        .thumb div {    
            width: 120px;
            height: 120px;
            display: inline-block;
            background-position: center center;
            background-size: cover;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
            border: 2px;
            border-color: #bbbbbb;
            border-radius: 4px;
            box-shadow: 0 0 2px 2px rgba(0, 140, 186, 0.7);
        }

        .samples-footer {
            width: 100%;
            padding: 10px;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.16),0 2px 10px 0 rgba(0,0,0,0.12) !important;
            background-color: #f1f1f1fa;
            display: inline-block;
        }
        .samples-footer a{
            float: right;
        }
	li {margin-bottom: 10px}

        .disabledLink {
            pointer-events:none;
        }

	@media (max-width: 400px) {
    		.center {
        		width: 100%;
    		}
	}

	ul {list-style-type: circle; }

	pre {
		background-color: rgb(246, 248, 250);
		border-bottom-left-radius: 3px;
		border-bottom-right-radius: 3px;
		border-top-left-radius: 3px;
		border-top-right-radius: 3px;
		box-sizing: border-box;
		color: rgb(36, 41, 46);
		font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
		font-size: 13.6px;
		line-height: 19.7167px;
		margin-bottom: 0px;
		margin-top: 20px;
		overflow: auto;
		overflow-wrap: normal;
		overflow-x: auto;
		overflow-y: auto;
		padding-left: 35px;
		padding-bottom: 35px;
		padding-right: 35px;
	}

	code {
		background-attachment: scroll;
		background-clip: border-box;
		background-color: rgba(0, 0, 0, 0);
		background-image: none;
		background-origin: padding-box;
		background-position: 0% 0%;
		background-position-x: 0%;
		background-position-y: 0%;
		background-repeat: repeat;
		background-size: auto;
		border-bottom-color: rgb(36, 41, 46);
		border-bottom-left-radius: 3px;
		border-bottom-right-radius: 3px;
		border-bottom-style: none;
		border-bottom-width: 0px;
		border-image-outset: 0;
		border-image-repeat: stretch;
		border-image-slice: 100%;
		border-image-source: none;
		border-image-width: 1;
		border-left-color: rgb(36, 41, 46);
		border-left-style: none;
		border-left-width: 0px;
		border-right-color: rgb(36, 41, 46);
		border-right-style: none;
		border-right-width: 0px;
		border-top-color: rgb(36, 41, 46);
		border-top-left-radius: 3px;
		border-top-right-radius: 3px;
		border-top-style: none;
		border-top-width: 0px;
		box-sizing: border-box;
		color: rgb(36, 41, 46);
		display: inline;
		font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
		font-size: 13.6px;
		line-height: 19.7167px;
		margin-bottom: 0px;
		margin-left: 0px;
		margin-right: 0px;
		margin-top: 0px;
		overflow: visible;
		overflow-wrap: normal;
		overflow-x: visible;
		overflow-y: visible;
		padding-bottom: 0px;
		padding-left: 0px;
		padding-right: 0px;
		padding-top: 0px;
		white-space: pre;
		word-break: normal;
	}
	</style>
</head>
<body>
    <div class="container">
        <h1>
            Virtual to Real adaptation of Pedestrian Detectors for Smart Cities<br>
            <small>
                <a href="https://scholar.google.it/citations?user=dCjyf-8AAAAJ&hl=it">Luca Ciampi</a>,
		<a href="https://scholar.google.it/citations?user=g-UGCd8AAAAJ&hl=en">Nicola Messina</a>,
                <a href="http://www.nmis.isti.cnr.it/falchi/">Fabrizio Falchi</a>,
                <a href="http://www.nmis.isti.cnr.it/gennaro/">Claudio Gennaro</a>,
		<a href="http://www.nmis.isti.cnr.it/amato/">Giuseppe Amato</a>,
            </small>
        </h1>
        
        
        <div class="abstract">
            <h2>Abstract</h2>
            <p> Pedestrian detection through computer vision is a building block for a multitude of applications in the context of smart cities, such as surveillance of sensitive areas, personal safety, monitoring, and control of pedestrian flow, to mention only a few. Recently, there was an increasing interest in deep learning architectures for performing such a task. One of the critical objectives of these algorithms is to generalize the knowledge gained during the training phase to new scenarios having various characteristics, and a suitably labeled dataset is fundamental to achieve this goal. The main problem is that manually annotating a dataset usually requires a lot of human effort, and it is a time-consuming operation. For this reason, in this work, we introduced <b>ViPeD</b> (Virtual Pedestrian Dataset), a new synthetically generated set of images collected from a realistic 3D video game where the labels can be automatically generated exploiting 2D pedestrian positions extracted from the graphics engine. We used this new synthetic dataset training a state-of-the-art computationally-efficient Convolutional Neural Network (CNN) that is ready to be installed in smart low-power devices, like smart cameras. We addressed the problem of the domain-adaptation from the virtual world to the real one by fine-tuning the CNN using the synthetic data and also exploiting a mixed-batch supervised training approach. Extensive experimentation carried out on different real-world datasets shows very competitive results compared to other methods presented in the literature in which the algorithms are trained using real-world data.
            </p>
        </div>
        
	<div class="container">
	  <div class="row">
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_1_bbs.png"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_2_bbs.png"></div>
	    <div class="w-100"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_3_bbs.png"></div>
	    <div class="col-6 col-md-6"><img class="img-thumbnail" src="example_4_bbs.png"></div>
	  </div> 
	</div>


        <div class="paper">
            <h2>Papers</h2>
		<ul>
			<li><b>Learning Pedestrian Detection from Virtual Worlds</b> (<a href="ICIAP__19___Learning_Pedestrian_Detection_from_Virtual_Worlds.pdf"> Download</a>, 3.3MB). The paper has been presented at <a href="https://event.unitn.it/iciap2019/">ICIAP 2019</a>.
			</li>
			<li><b>Virtual to Real adaptation of Pedestrian Detectors for Smart Cities</b> <a href="https://arxiv.org/pdf/2001.03032.pdf"> (ArXiv Download</a>, 7.7MB). The paper has been submitted to the <a href="https://www.journals.elsevier.com/neurocomputing/call-for-papers/special-issue-on-advances-of-neurocomputing-for-smart-cities">Special Issue on Advances of Neurocomputing for Smart Cities</a>. This is the extended version of the first paper. It contains a wider experimental section and introduces experiments with a different supervised domain-adaptation approach.
			</li>
		</ul>
        </div>

        <div class="dataset">
            <h2>Dataset and Code</h2>
                <p>This dataset is an extension of the <a href="http://imagelab.ing.unimore.it/jta">JTA</a> (Joint Track Auto) dataset.</p>
		<p>
                <a href="http://aimir.isti.cnr.it/viped/">Here</a> you can find more details about the ViPeD dataset, together with the download link.
                </p>
		<p> The code for training and evaluating Faster-RCNN with our method is available in our <a href="https://github.com/ciampluca/Virtual-to-Real-Pedestrian-Detection">GitHub repository</a>.
        </div>
        <div class="cite">
	    <h2>Cite our work</h2>
		If you find this work or code useful for your research, please cite the following:
		<pre><code>
@misc{ciampi2020virtual,
    title={Virtual to Real adaptation of Pedestrian Detectors for Smart Cities},
    author={Luca Ciampi and Nicola Messina and Fabrizio Falchi and Claudio Gennaro and Giuseppe Amato},
    year={2020},
    eprint={2001.03032},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@inproceedings{amato2019learning,
  title={Learning pedestrian detection from virtual worlds},
  author={Amato, Giuseppe and Ciampi, Luca and Falchi, Fabrizio and Gennaro, Claudio and Messina, Nicola},
  booktitle={International Conference on Image Analysis and Processing},
  pages={302--312},
  year={2019},
  organization={Springer}
}</code></pre>
	</div>
        <div class="acks">
            <p>
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Jetson TX2 used for this research.</p>
        </div>
    </div>
</body>
</html>

