<!DOCTYPE html>
<html lang="en" ng-app="AdvApp">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Learning Pedestrian Detection from Virtual Worlds</title>
    
    <!-- Latest compiled and minified JavaScript -->
    <script src="https://code.jquery.com/jquery-1.12.4.min.js" integrity="sha256-ZosEbRLbNQzLpnKIkEdrPv7lOy9C27hHQ+Xp8a4MxAQ=" crossorigin="anonymous"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/angularjs/1.6.4/angular.min.js" crossorigin="anonymous"></script>
    <script src="ng-infinite-scroll.min.js" type="application/javascript"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    
    <!--
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-97320150-1', 'auto');
    ga('send', 'pageview'); 

    
    </script> -->
    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    
    <!-- Latest compiled and minified CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    <!-- Optional theme -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">    
    <!-- Fonts! -->
    <link href="http://fonts.googleapis.com/css?family=Roboto:400,300" rel="stylesheet" type="text/css">
    
    <style>
        body {
            font-family: 'Roboto', sans-serif;
            font-weight: 200;
            font-size: 16px;
            text-align: justify;
        }
        h1 { text-align: center; margin-top: 20px }
        .container > * { margin-bottom: 50px }
        #table-container { margin-top: 50px; height: 600px; overflow: auto; }
        #table-container tbody td { vertical-align: middle; }
        #table-container tbody td.class-text { width: 27%; text-align: left; }
        .score { font-weight: bold; text-align: center; }
        .thumb div {    
            width: 120px;
            height: 120px;
            display: inline-block;
            background-position: center center;
            background-size: cover;
        }

        .center {
            display: block;
            margin-left: auto;
            margin-right: auto;
            width: 50%;
            border: 2px;
            border-color: #bbbbbb;
            border-radius: 4px;
            box-shadow: 0 0 2px 2px rgba(0, 140, 186, 0.7);
        }

        .samples-footer {
            width: 100%;
            padding: 10px;
            box-shadow: 0 2px 4px 0 rgba(0,0,0,0.16),0 2px 10px 0 rgba(0,0,0,0.12) !important;
            background-color: #f1f1f1fa;
            display: inline-block;
        }
        .samples-footer a{
            float: right;
        }

        .disabledLink {
            pointer-events:none;
        }

	@media (max-width: 400px) {
    		.center {
        		width: 100%;
    		}
}
}

    </style>
</head>
<body>
    <div class="container">
        <h1>
            Learning Pedestrian Detection from Virtual Worlds<br>
            <small>
                <a href="http://www.nmis.isti.cnr.it/amato/">Giuseppe Amato</a>,
                <a href="https://scholar.google.it/citations?user=dCjyf-8AAAAJ&hl=it">Luca Ciampi</a>,
                <a href="http://www.nmis.isti.cnr.it/falchi/">Fabrizio Falchi</a>,
                <a href="http://www.nmis.isti.cnr.it/gennaro/">Claudio Gennaro</a>,
                <a href="https://scholar.google.it/citations?user=g-UGCd8AAAAJ&hl=en">Nicola Messina</a>
            </small>
        </h1>
        
        
        <div class="abstract">
            <h2>Abstract</h2>
            <p> In this paper, we present a real-time pedestrian detection system that has been trained using a virtual environment. This is a very popular topic of research having endless practical applications 
and recently, there was an increasing interest in deep learning architectures for performing such a task. However, the availability of large labeled datasets is a key point for an effective train of such algorithms. 
For this reason, in this work, we introduced <b>ViPeD</b>, a new synthetically generated set of images extracted from a realistic 3D video game where the labels can be automatically generated exploiting 2D pedestrian positions extracted from the graphics engine. We exploited this new synthetic dataset fine-tuning a state-of-the-art computationally efficient Convolutional Neural Network (CNN). 
A preliminary experimental evaluation, compared to the performance of other existing approaches trained on real-world images, shows encouraging results.
            </p>
        </div>
        <img class="img-thumbnail center" ng-src="clevr-imgs/train/CLEVR_train_000004.png" src="viped_example.png" width="50%">

        <div class="paper">
            <h2>Paper <small>(<a href="ICIAP__19___Learning_Pedestrian_Detection_from_Virtual_Worlds.pdf">Preprint PDF</a>, 3.3MB)</small></h2>
            The paper has been presented at <a href="https://event.unitn.it/iciap2019/">ICIAP 2019</a> .
        </div>

        <div class="dataset">
            <h2>Dataset</h2>
		<h3>(section under construction)</h3>
                <p>This dataset is an extension of the <a href="http://imagelab.ing.unimore.it/jta">JTA</a> (Joint Track Auto) dataset.
                For this reason, we don't publish the images directly; we instead release the annotations and the Python code used for the image augmentation process. </p>
                <p>
                <a href="http://aimir.isti.cnr.it/viped/">Here</a> you can find more details about the ViPeD dataset, together with links to the code and the annotations.
                </p>
        </div>
        
        <div class="acks">
            <p>
We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.</p>
        </div>
    </div>
</body>
</html>

